{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/clutch/anaconda3/envs/insight/lib/python3.7/site-packages/sqlalchemy/sql/functions.py:68: SAWarning: The GenericFunction 'array_agg' is already registered and is going to be overriden.\n",
      "  \"is going to be overriden.\".format(identifier))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import collections\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "import json\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy_utils import create_database, database_exists, drop_database\n",
    "import psycopg2\n",
    "from psycopg2.extensions import ISOLATION_LEVEL_AUTOCOMMIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using function from \"Keeping API Keys Secret.ipynb\"\n",
    "# by https://github.com/dylburger\n",
    "# def get_file_contents(filename):\n",
    "#     \"\"\" \n",
    "#     Given a filename,\n",
    "#     return the contents of that file\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         with open(filename, 'r') as f:\n",
    "#             # assumed file is a single line with key\n",
    "#             return f.read().strip()\n",
    "#     except FileNotFoundError:\n",
    "#         print(\"'%s' file not found\" % filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a database name (we're using a dataset on births, so we'll call it birth_db)\n",
    "# Set your postgres username\n",
    "# dbname = 'insight_db'\n",
    "# username = 'postgres' # change this to your username\n",
    "# password = get_file_contents('../keys/psql_key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'engine' is a connection to a database\n",
    "# engine = create_engine('postgres://%s:%s@localhost/%s'%(username,password,dbname))\n",
    "# # load sql_magic so we can write SQL in Jupyter Notebooks\n",
    "# %load_ext sql_magic\n",
    "\n",
    "# # setup SQL connection to the postgreSQL engine we created\n",
    "# %config SQL.conn_name = 'engine'\n",
    "# print(engine.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to make queries using psycopg2\n",
    "# con = psycopg2.connect(database = dbname, host = 'localhost', user = username, password = password)\n",
    "# con.set_isolation_level(ISOLATION_LEVEL_AUTOCOMMIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages to perform Text Summarization on Youtube Video Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a List of btopwords\n",
    "stopwords = list(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sql query to grab all comments and store in dataframe\n",
    "# sql = \"SELECT * \\\n",
    "#        FROM comments\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_df = pd.read_sql(sql, con=con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_csv_filename = 'data/csv_files/original_data.csv'\n",
    "data_df = pd.read_csv(from_csv_filename, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged comments by video_id\n",
    "# comments_merged = pd.DataFrame(data_df.groupby('video_id').text.apply(lambda x: \", \".join(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load spacy english language\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build an nlp document for video_id = K7tnCDXa-rY\n",
    "video_doc_example = data_df[data_df.video_id=='P4D0sESi5So'].text.item()\n",
    "nlp_obj = nlp(video_doc_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization of text\n",
    "mytokens = [token.text for token in nlp_obj]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build word frequency \n",
    "# word.text is tokenization in spacy\n",
    "word_frequencies = {}\n",
    "for word in nlp_obj:\n",
    "    if word.text not in stopwords:\n",
    "            if word.text not in word_frequencies.keys():\n",
    "                word_frequencies[word.text] = 1\n",
    "            else:\n",
    "                word_frequencies[word.text] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maximum word frequency\n",
    "maximum_frequency = max(word_frequencies.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in word_frequencies.keys():\n",
    "    word_frequencies[word] = (word_frequencies[word]/maximum_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency table\n",
    "#word_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence tokens\n",
    "sentence_list = [sentence for sentence in nlp_obj.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of sentence tokenization, word tokenization and lowering All Text\n",
    "# for t in sentence_list:\n",
    "#     for w in t:\n",
    "#         print(w.text.lower())\n",
    "#[w.text.lower() for t in sentence_list for w in t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence Score via comparrng each word with sentence\n",
    "sentence_scores = {}  \n",
    "for sent in sentence_list:  \n",
    "        for word in sent:\n",
    "            if word.text.lower() in word_frequencies.keys():\n",
    "                if len(sent.text.split(' ')) < 30:\n",
    "                    if sent not in sentence_scores.keys():\n",
    "                        sentence_scores[sent] = word_frequencies[word.text.lower()]\n",
    "                    else:\n",
    "                        sentence_scores[sent] += word_frequencies[word.text.lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence Score via comparrng each word with sentence\n",
    "# Alternative Method\n",
    "lowered_sentence_list = [w.text.lower() for t in sentence_list for w in t ]\n",
    "lowered_sentence_scores = {}  \n",
    "for sent in lowered_sentence_list:  \n",
    "        for word in sent:\n",
    "            if word in word_frequencies.keys():\n",
    "                if len(sent.split(' ')) < 30:\n",
    "                    if sent not in sentence_scores.keys():\n",
    "                        lowered_sentence_scores[sent] = word_frequencies[word]\n",
    "                    else:\n",
    "                        lowered_sentence_scores[sent] += word_frequencies[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence score table\n",
    "#sentence_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the Top N sentences with largest score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from heapq import nlargest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarized_sentences = nlargest(15, sentence_scores, key=sentence_scores.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarized_sentences_cleaned = []\n",
    "for ss in summarized_sentences:\n",
    "    if ss.text.find(', ')==0:\n",
    "        idx = ss.text.find(', ')\n",
    "        new_str = ss[idx+1:].text.capitalize()\n",
    "    else:\n",
    "        new_str = ss.text.capitalize()\n",
    "    new_str = ' '.join(new_str.split())\n",
    "    summarized_sentences_cleaned.append(new_str) \n",
    "summarized_sentences_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert sentences from spacy span to strings for joining entire sentence\n",
    "final_sentences = []\n",
    "for w in summarized_sentences_cleaned:\n",
    "    for i, a in enumerate(w.text):\n",
    "        if not a.isalpha() and a.isspace():\n",
    "            break\n",
    "    final_sentences.append(w.text[i:].capitalize())\n",
    "    #print(w.text[i:].capitalize())\n",
    "#    print(w.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list comprehension of sentences converted from spacy.span to strings\n",
    "final_sentences = [ w.text for w in summarized_sentences ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = ' '.join(summarized_sentences_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = summary.replace(u'\\xa0', u'')\n",
    "summary = summary.replace(' , ', ' ')\n",
    "summary = ' '.join(summary.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(video_doc_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare to Gensim Summarization Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.summarization import summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim_summary = summarize(video_doc_example, ratio=0.009)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gensim_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to use in app.py for Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from string import punctuation\n",
    "\n",
    "# build a list of stopwords\n",
    "stopwords = list(STOP_WORDS)\n",
    "\n",
    "def text_summarizer(document):\n",
    "    raw_text = document\n",
    "    doc = nlp(raw_text)\n",
    "    # build word frequency\n",
    "    # word.text is tokenization in spacy\n",
    "    word_frequencies = {}  \n",
    "    for word in doc:  \n",
    "        if word.text not in stopwords:\n",
    "            if word.text not in word_frequencies.keys():\n",
    "                word_frequencies[word.text] = 1\n",
    "            else:\n",
    "                word_frequencies[word.text] += 1\n",
    "\n",
    "    maximum_frequncy = max(word_frequencies.values())\n",
    "\n",
    "    for word in word_frequencies.keys():\n",
    "        word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)\n",
    "    # sentence tokens\n",
    "    sentence_list = [ sentence for sentence in doc.sents ]\n",
    "\n",
    "    # calculate sentence score and ranking\n",
    "    sentence_scores = {}  \n",
    "    for sent in sentence_list:  \n",
    "        for word in sent:\n",
    "            if word.text.lower() in word_frequencies.keys():\n",
    "                if len(sent.text.split(' ')) < 30:\n",
    "                    if sent not in sentence_scores.keys():\n",
    "                        sentence_scores[sent] = word_frequencies[word.text.lower()]\n",
    "                    else:\n",
    "                        sentence_scores[sent] += word_frequencies[word.text.lower()]\n",
    "\n",
    "    # find N largest\n",
    "    summary_sentences = nlargest(5, sentence_scores, key=sentence_scores.get)\n",
    "    \n",
    "    final_sentences = []\n",
    "    for w in summary_sentences:\n",
    "        for i, a in enumerate(w.text):\n",
    "            if a.isalpha():\n",
    "                break\n",
    "        final_sentences.append(w.text[i:].capitalize())\n",
    "    \n",
    "    summary = ' '.join(final_sentences)\n",
    "    summary = summary.replace(u'\\xa0', u'')\n",
    "    print(\"Original Document\\n\")\n",
    "    print(document)\n",
    "    print(\"Total Length:\",len(document))\n",
    "    print('\\n\\nSummarized Document\\n')\n",
    "    print(summary)\n",
    "    print(\"Total Length:\",len(summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_summarizer(video_doc_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
